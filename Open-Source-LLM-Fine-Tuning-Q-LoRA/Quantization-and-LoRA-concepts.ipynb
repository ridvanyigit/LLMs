{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1301f299",
   "metadata": {},
   "source": [
    "## Predict Product Prices\n",
    "\n",
    "An introduction to LoRA and QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34da8d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip installs\n",
    "\n",
    "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0\n",
    "!pip install -q datasets requests peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1989d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, set_seed\n",
    "from peft import LoraConfig, PeftModel\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b622565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "FINETUNED_MODEL = f\"ed-donner/pricer-2024-09-13_13.04.39\"\n",
    "\n",
    "# Hyperparameters for QLoRA Fine-Tuning\n",
    "\n",
    "LORA_R = 32\n",
    "LORA_ALPHA = 64\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100c93f6",
   "metadata": {},
   "source": [
    "### Log in to HuggingFace\n",
    "\n",
    "If you don't already have a HuggingFace account, visit https://huggingface.co to sign up and create a token.\n",
    "\n",
    "Then select the Secrets for this Notebook by clicking on the key icon in the left, and add a new secret called `HF_TOKEN` with the value as your token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eec2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to Hugging Face\n",
    "\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895bfd80",
   "metadata": {},
   "source": [
    "## Trying out different Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Base Model without quantization\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcaa3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7ed2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95119c0e",
   "metadata": {},
   "source": [
    "## Restart your session!\n",
    "\n",
    "In order to load the next model and clear out the cache of the last model, you'll now need to go to Runtime >> Restart session and run the initial cells (installs and imports and HuggingFace login) again.\n",
    "\n",
    "This is to clean out the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603f32eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Base Model using 8 bit\n",
    "\n",
    "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c18e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3241d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1ba16",
   "metadata": {},
   "source": [
    "## Restart your session!\n",
    "\n",
    "In order to load the next model and clear out the cache of the last model, you'll now need to go to Runtime >> Restart session and run the initial cells (imports and HuggingFace login) again.\n",
    "\n",
    "This is to clean out the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a7bb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Tokenizer and the Base Model using 4 bit\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c462fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70af9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bbc3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac2af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Memory footprint: {fine_tuned_model.get_memory_footprint() / 1e9:,.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1e1923",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f51c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each of the Target Modules has 2 LoRA Adaptor matrices, called lora_A and lora_B\n",
    "# These are designed so that weights can be adapted by adding alpha * lora_A * lora_B\n",
    "# Let's count the number of weights using their dimensions:\n",
    "\n",
    "# See the matrix dimensions above\n",
    "lora_q_proj = 4096 * 32 + 4096 * 32\n",
    "lora_k_proj = 4096 * 32 + 1024 * 32\n",
    "lora_v_proj = 4096 * 32 + 1024 * 32\n",
    "lora_o_proj = 4096 * 32 + 4096 * 32\n",
    "\n",
    "# Each layer comes to\n",
    "lora_layer = lora_q_proj + lora_k_proj + lora_v_proj + lora_o_proj\n",
    "\n",
    "# There are 32 layers\n",
    "params = lora_layer * 32\n",
    "\n",
    "# So the total size in MB is\n",
    "size = (params * 4) / 1_000_000\n",
    "\n",
    "print(f\"Total number of params: {params:,} and size {size:,.1f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30fff33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
