{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321e538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip installs - ignore the error message!\n",
    "\n",
    "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install -q --upgrade requests==2.32.3 bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 datasets==3.2.0 peft==0.14.0 trl==0.14.0 matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa30f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, set_seed\n",
    "from peft import LoraConfig, PeftModel\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddc3752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizers\n",
    "\n",
    "LLAMA_3_1 = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "QWEN_2_5 = \"Qwen/Qwen2.5-7B\"\n",
    "GEMMA_2 = \"google/gemma-2-9b\"\n",
    "PHI_3 = \"microsoft/Phi-3-medium-4k-instruct\"\n",
    "\n",
    "# Constants\n",
    "\n",
    "BASE_MODEL = LLAMA_3_1\n",
    "HF_USER = \"ed-donner\"\n",
    "DATASET_NAME = f\"{HF_USER}/pricer-data\"\n",
    "MAX_SEQUENCE_LENGTH = 182\n",
    "QUANT_4_BIT = True\n",
    "\n",
    "# Used for writing to output in color\n",
    "\n",
    "GREEN = \"\\033[92m\"\n",
    "YELLOW = \"\\033[93m\"\n",
    "RED = \"\\033[91m\"\n",
    "RESET = \"\\033[0m\"\n",
    "COLOR_MAP = {\"red\":RED, \"orange\": YELLOW, \"green\": GREEN}\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ce8276",
   "metadata": {},
   "source": [
    "### Log in to HuggingFace\n",
    "\n",
    "If you don't already have a HuggingFace account, visit https://huggingface.co to sign up and create a token.\n",
    "\n",
    "Then select the Secrets for this Notebook by clicking on the key icon in the left, and add a new secret called `HF_TOKEN` with the value as your token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9d4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to HuggingFace\n",
    "\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fb4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_tokenizer(model_name):\n",
    "  print(\"Investigating tokenizer for\", model_name)\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "  for number in [0, 1, 10, 100, 999, 1000]:\n",
    "    tokens = tokenizer.encode(str(number), add_special_tokens=False)\n",
    "    print(f\"The tokens for {number}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f490b91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will try this with each model: LLAMA_3_1, QWEN_2_5, GEMMA_2, PHI_3\n",
    "\n",
    "investigate_tokenizer(PHI_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9094c8",
   "metadata": {},
   "source": [
    "# Load our data\n",
    "\n",
    "We uploaded it to Hugging Face, so it's easy to retrieve it now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30078f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME)\n",
    "train = dataset['train']\n",
    "test = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b3374",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33093ab6",
   "metadata": {},
   "source": [
    "# Prepare our Base Llama Model for evaluation\n",
    "\n",
    "Load our base model with 4 bit quantization and try out 1 example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58a0ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick the right quantization\n",
    "\n",
    "if QUANT_4_BIT:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    "  )\n",
    "else:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d0c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Tokenizer and the Model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5966e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_price(s):\n",
    "    if \"Price is $\" in s:\n",
    "      contents = s.split(\"Price is $\")[1]\n",
    "      contents = contents.replace(',','').replace('$','')\n",
    "      match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", contents)\n",
    "      return float(match.group()) if match else 0\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4edf114",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_price(\"Price is $999 blah blah so cheap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949ac745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(prompt):\n",
    "    set_seed(42)\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    attention_mask = torch.ones(inputs.shape, device=\"cuda\")\n",
    "    outputs = base_model.generate(inputs, max_new_tokens=4, attention_mask=attention_mask, num_return_sequences=1)\n",
    "    response = tokenizer.decode(outputs[0])\n",
    "    return extract_price(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict(test[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a03555f",
   "metadata": {},
   "source": [
    "# Evaluation!\n",
    "\n",
    "Trying out our base Llama 3.1 model against the Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b570eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester:\n",
    "\n",
    "    def __init__(self, predictor, data, title=None, size=250):\n",
    "        self.predictor = predictor\n",
    "        self.data = data\n",
    "        self.title = title or predictor.__name__.replace(\"_\", \" \").title()\n",
    "        self.size = size\n",
    "        self.guesses = []\n",
    "        self.truths = []\n",
    "        self.errors = []\n",
    "        self.sles = []\n",
    "        self.colors = []\n",
    "\n",
    "    def color_for(self, error, truth):\n",
    "        if error<40 or error/truth < 0.2:\n",
    "            return \"green\"\n",
    "        elif error<80 or error/truth < 0.4:\n",
    "            return \"orange\"\n",
    "        else:\n",
    "            return \"red\"\n",
    "\n",
    "    def run_datapoint(self, i):\n",
    "        datapoint = self.data[i]\n",
    "        guess = self.predictor(datapoint[\"text\"])\n",
    "        truth = datapoint[\"price\"]\n",
    "        error = abs(guess - truth)\n",
    "        log_error = math.log(truth+1) - math.log(guess+1)\n",
    "        sle = log_error ** 2\n",
    "        color = self.color_for(error, truth)\n",
    "        title = datapoint[\"text\"].split(\"\\n\\n\")[1][:20] + \"...\"\n",
    "        self.guesses.append(guess)\n",
    "        self.truths.append(truth)\n",
    "        self.errors.append(error)\n",
    "        self.sles.append(sle)\n",
    "        self.colors.append(color)\n",
    "        print(f\"{COLOR_MAP[color]}{i+1}: Guess: ${guess:,.2f} Truth: ${truth:,.2f} Error: ${error:,.2f} SLE: {sle:,.2f} Item: {title}{RESET}\")\n",
    "\n",
    "    def chart(self, title):\n",
    "        max_error = max(self.errors)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        max_val = max(max(self.truths), max(self.guesses))\n",
    "        plt.plot([0, max_val], [0, max_val], color='deepskyblue', lw=2, alpha=0.6)\n",
    "        plt.scatter(self.truths, self.guesses, s=3, c=self.colors)\n",
    "        plt.xlabel('Ground Truth')\n",
    "        plt.ylabel('Model Estimate')\n",
    "        plt.xlim(0, max_val)\n",
    "        plt.ylim(0, max_val)\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    def report(self):\n",
    "        average_error = sum(self.errors) / self.size\n",
    "        rmsle = math.sqrt(sum(self.sles) / self.size)\n",
    "        hits = sum(1 for color in self.colors if color==\"green\")\n",
    "        title = f\"{self.title} Error=${average_error:,.2f} RMSLE={rmsle:,.2f} Hits={hits/self.size*100:.1f}%\"\n",
    "        self.chart(title)\n",
    "\n",
    "    def run(self):\n",
    "        self.error = 0\n",
    "        for i in range(self.size):\n",
    "            self.run_datapoint(i)\n",
    "        self.report()\n",
    "\n",
    "    @classmethod\n",
    "    def test(cls, function, data):\n",
    "        cls(function, data).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester.test(model_predict, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a78941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
